{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"800px\" src=\"../fidle/img/00-Fidle-header-01.svg\"></img>\n",
    "\n",
    "# <!-- TITLE --> [SYNOP2] - First predictions at 3h\n",
    "<!-- DESC --> Episode 2 : RNN training session for weather prediction attempt at 3h\n",
    "<!-- AUTHOR : Jean-Luc Parouty (CNRS/SIMaP) -->\n",
    "\n",
    "## Objectives :\n",
    " - Make a simple prediction (3h)\n",
    " - Understanding the use of a recurrent neural network\n",
    "\n",
    "\n",
    "SYNOP meteorological data, available at: https://public.opendatasoft.com\n",
    "\n",
    "## What we're going to do :\n",
    "\n",
    " - Read our dataset\n",
    " - Select our data and normalize it\n",
    " - Doing our training\n",
    " - Making simple predictions\n",
    "\n",
    "## Step 1 - Import and init\n",
    "### 1.1 - Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "\n",
    "import numpy as np\n",
    "import math, random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import h5py, json\n",
    "import os,time,sys\n",
    "\n",
    "from importlib import reload\n",
    "\n",
    "sys.path.append('..')\n",
    "import fidle.pwk as pwk\n",
    "\n",
    "run_dir = './run/SYNOP'\n",
    "datasets_dir = pwk.init('SYNOP2', run_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 - Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- About dataset (no need to change)\n",
    "#\n",
    "dataset_dir      = './data'                  # Enhanced dataset is very small, so ./data in a good choice :-)\n",
    "dataset_filename = 'synop-LYS.csv'\n",
    "schema_filename  = 'synop.json'\n",
    "features         = ['tend', 'cod_tend', 'dd', 'ff', 'td', 'u', 'ww', 'pres', 'rafper', 'rr1', 'rr3', 'tc']\n",
    "features_len     = len(features)\n",
    "\n",
    "# ---- About training (Can be changed !)\n",
    "#\n",
    "scale            = 1        # Percentage of dataset to be used (1=all)\n",
    "train_prop       = .8       # Percentage for train (the rest being for the test)\n",
    "sequence_len     = 16\n",
    "batch_size       = 32\n",
    "epochs           = 10\n",
    "fit_verbosity    = 1        # 0 = silent, 1 = progress bar, 2 = one line per epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Override parameters (batch mode) - Just forget this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwk.override('scale', 'train_prop', 'sequence_len', 'batch_size', 'epochs', 'fit_verbosity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Read and prepare dataset\n",
    "### 2.1 - Read it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Read dataset from ./data\n",
    "\n",
    "df = pd.read_csv(f'{dataset_dir}/{dataset_filename}', header=0, sep=';')\n",
    "\n",
    "# ---- Scaling\n",
    "\n",
    "df = df[:int(scale*len(df))]\n",
    "train_len=int(train_prop*len(df))\n",
    "\n",
    "# ---- Train / Test\n",
    "dataset_train = df.loc[ :train_len-1, features ]\n",
    "dataset_test  = df.loc[train_len:,    features ]\n",
    "pwk.subtitle('Train dataset example :')\n",
    "display(dataset_train.head(15))\n",
    "\n",
    "# ---- Normalize, and convert to numpy array\n",
    "\n",
    "mean = dataset_train.mean()\n",
    "std  = dataset_train.std()\n",
    "dataset_train = (dataset_train - mean) / std\n",
    "dataset_test  = (dataset_test  - mean) / std\n",
    "\n",
    "pwk.subtitle('After normalization :')\n",
    "display(dataset_train.describe().style.format(\"{0:.2f}\"))\n",
    "\n",
    "dataset_train = dataset_train.to_numpy()\n",
    "dataset_test  = dataset_test.to_numpy()\n",
    "\n",
    "pwk.subtitle('Shapes :')\n",
    "print('Dataset       : ',df.shape)\n",
    "print('Train dataset : ',dataset_train.shape)\n",
    "print('Test  dataset : ',dataset_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - Prepare data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Train generator\n",
    "train_generator = TimeseriesGenerator(dataset_train, dataset_train, length=sequence_len,  batch_size=batch_size)\n",
    "test_generator  = TimeseriesGenerator(dataset_test,  dataset_test,  length=sequence_len,  batch_size=batch_size)\n",
    "\n",
    "# ---- About\n",
    "\n",
    "pwk.subtitle('About the splitting of our dataset :')\n",
    "\n",
    "x,y=train_generator[0]\n",
    "print(f'Nombre de train batchs disponibles : ', len(train_generator))\n",
    "print('batch x shape : ',x.shape)\n",
    "print('batch y shape : ',y.shape)\n",
    "\n",
    "x,y=train_generator[0]\n",
    "pwk.subtitle('What a batch looks like (x) :')\n",
    "pwk.np_print(x[0] )\n",
    "pwk.subtitle('What a batch looks like (y) :')\n",
    "pwk.np_print(y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Create a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add( keras.layers.InputLayer(input_shape=(sequence_len, features_len)) )\n",
    "model.add( keras.layers.LSTM(100, activation='relu') )\n",
    "model.add( keras.layers.Dropout(0.2) )\n",
    "model.add( keras.layers.Dense(features_len) )\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Compile and train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 - Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwk.mkdir(run_dir)\n",
    "save_dir = f'{run_dir}/best_model.h5'\n",
    "bestmodel_callback = tf.keras.callbacks.ModelCheckpoint(filepath=save_dir, verbose=0, save_best_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 - Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', \n",
    "              loss='mse', \n",
    "              metrics   = ['mae'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 - Fit\n",
    "6' with a CPU (laptop)  \n",
    "2' with a GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwk.chrono_start()\n",
    "\n",
    "history=model.fit(train_generator, \n",
    "                  epochs  = epochs, \n",
    "                  verbose = fit_verbosity,\n",
    "                  validation_data = test_generator,\n",
    "                  callbacks = [bestmodel_callback])\n",
    "\n",
    "pwk.chrono_show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwk.plot_history(history,plot={'loss':['loss','val_loss'], 'mae':['mae','val_mae']}, save_as='01-history')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 - Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 - Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = tf.keras.models.load_model(f'{run_dir}/best_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Make a prediction\n",
    "A basic prediction, with normalized values (so humanly not very understandable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=random.randint(0,len(dataset_test)-sequence_len)\n",
    "\n",
    "sequence      = dataset_test[s:s+sequence_len]\n",
    "sequence_true = dataset_test[s:s+sequence_len+1]\n",
    "\n",
    "pred = loaded_model.predict( np.array([sequence]) )\n",
    "\n",
    "# ---- Show result\n",
    "pwk.plot_multivariate_serie(sequence_true, predictions=pred, labels=features, save_as='02-prediction-norm')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Real prediction\n",
    "We are now going to make a true prediction, with an un-normalized result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(mean,std,seq):\n",
    "    nseq = seq.copy()\n",
    "    for i,s in enumerate(nseq):\n",
    "        s = s*std + mean\n",
    "        nseq[i]=s\n",
    "    return nseq\n",
    "\n",
    "\n",
    "# ---- Get a sequence\n",
    "\n",
    "i=random.randint(0,len(dataset_test)-sequence_len)\n",
    "sequence      = dataset_test[i:i+sequence_len]\n",
    "sequence_true = dataset_test[i:i+sequence_len+1]\n",
    "\n",
    "# ---- Prediction\n",
    "\n",
    "pred = loaded_model.predict( np.array([sequence]) )\n",
    "\n",
    "# ---- De-normalization\n",
    "\n",
    "sequence_true = denormalize(mean,std, sequence_true)\n",
    "pred          = denormalize(mean,std, pred)\n",
    "\n",
    "# ---- Show it\n",
    "feat=11\n",
    "\n",
    "pwk.plot_multivariate_serie(sequence_true, predictions=pred, labels=features, only_features=[feat],width=14, height=8, save_as='03-prediction')\n",
    "\n",
    "delta_deg=abs(sequence_true[-1][feat]-pred[-1][feat])\n",
    "print(f'Gap between prediction and reality : {delta_deg:.2f} °C')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwk.end()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<img width=\"80px\" src=\"../fidle/img/00-Fidle-logo-01.svg\"></img>"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8e38643e33497db9a306e3f311fa98cb1e65371278ca73ee4ea0c76aa5a4f387"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('fidle-cpu': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
